\section{Difficulties using NSI circuits}

\subsection{Circuit extent and capability}
NSI circuits do not extend all the way to the storage nodes of a site, they stop at the border router. This leaves the classic 'last mile' problem of maintaining performance end-to-end in any given transfer.

NSI does not necessarily provide bandwidth guarantees. This is not a show-stopper in itself, but it does greatly limit the value of the circuit. Even a soft guarantee (e.g. time-averaged performance) would be acceptable\cite{BW_sharing}, but the total lack of guarantee in some situations is a serious limitation which we hope the network providers will address in the near future.

NSI is also not widely adopted yet, and availability as a production service is limited. However, it is supported by a lot of large network providers, so availability is expected to grow over time.

\subsection{Layer 2 vs. Layer 3 circuits}

There is one major difference between the circuits provided by DYNES and the circuits provided by NSI that poses a serious problem for our use of circuits in PhEDEx. DYNES provides layer-3 circuits, i.e. circuits at the IP level. NSI only provides layer-2 circuits, at the ethernet level.

This is a problem because PhEDEx is high-level middleware, and knows only about source and destination hostnames and port numbers. It knows nothing about low-level details of the network such as ethernet addresses or anything to do with the topology of the network, it merely calls a transfer-layer tool (such as FTS\cite{FTS} or FDT\cite{FDT}) to handle the actual transfer. These tools take their input in the form of a SURL (a `Storage URL') which specifies the host, port, protocol and local pathname on that host to access the file. PhEDEx is able to switch between different layer-3, IP-based circuits by changing the hostname (or IP address) that it uses in the SURL, the rest happens transparently.

Because NSI doesn't provide layer-3 circuits, this mechanism is insufficient in itself, we need to create a layer-3 circuit on top of the layer-2 circuit. Unfortunately, this requires priviledged access to the sites' networking infrastructure, and sites are naturally reluctant to allow intrusive access of this nature.

The problem with creating a level-3 circuit on top of a level-2 circuit has essentially two components. First, creating a domain-wide level-3 circuit is not difficult, either technically or politically, indeed this is what DYNES provided in the past. In the absence of DYNES, or some agreed replacement for it, we need to create circuits triggered by the demands of the experiments middleware. This requires routing and topological information, something which PhEDEx has no knowledge of, and indeed PhEDEx should not know such things.

The second component of the problem is that domain-wide circuits may not be appropriate, in that they may accidentally allow traffic which should not be on the circuit to use it, thereby competing with the transfer we want to prioritise. During Run-2 of the LHC this is not likely to be such a problem, email and web traffic is hardly likely to compete with multi-terabyte data transfers. Even inter-experiment competition is not likely to be large because most places where CMS and ATLAS share sites are already well provisioned anyway (Tier-1s or larger Tier-2s). Circuits are more likely to be useful at the edges of the experiments' networks, and there they share fewer sites, so competition between experiments is likely to be lower. Within an experiment such as CMS, if we have multiple flows of data between two sites and we wish to prioritise one over the other, we can simply suspend the lower priority flow at the level of PhEDEx, so we don't need a circuit-level interface to manage that yet. There is still the possibility of competition between different data-management systems within CMS, e.g. between PhEDEx and AAA\cite{AAA} (the CMS implementation of an xrootd\cite{xrootd} federation) or ASO\cite{ASO} (the system for re-patriating user-files output from analysis jobs around the grid). To first order this is also expected to be small during Run-2, and we ignore it for now.

However, one of the goals of ANSE is not just to provide tools for CMS and ATLAS, but to provide tools for the broader science community. There are many experiments being built that will have significant networking needs and which could benefit from this work. Therefore this problem cannot be simply ignored, and needs to be addressed.